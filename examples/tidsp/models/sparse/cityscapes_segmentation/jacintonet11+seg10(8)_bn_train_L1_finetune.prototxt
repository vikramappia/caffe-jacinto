#Sover parameters
test_iter: 200
test_interval: 2000
test_initialization: true
display: 100

type: "Adam"
base_lr: 1e-5

lr_policy: "fixed"
gamma: 0.1
max_iter: 32000
momentum: 0.9

weight_decay: 1e-5
regularization_type: "L1"

snapshot: 1000
snapshot_prefix: "training/jacintonet11+seg10_train_L1_bn_finetune"
solver_mode: GPU
random_seed: 33

#snapshot_log: true
#ignore_mismatching_blobs: true

display_sparsity: 1000
#sparsity_threshold: 1e-4
#threshold_weights: true
weight_connect_mode: WEIGHT_DISCONNECTED_ELTWISE

#insert_quantization_param: true

#Net parameters
net_param {

name: "ConvNet-11+seg10(8)"

layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  include {  phase: TRAIN  }
  transform_param {
    crop_size: 640
    ##mirror: true    
    ##mean_value: 128
    ##mean_value: 128
    ##mean_value: 128
    #num_labels: 5
    #display: true
  }
  image_label_data_param {    
    image_list_path: "data/train-image-list.txt"
    label_list_path: "data/train-label-list.txt"
    batch_size: 16
    shuffle: true 
    label_slice {
      dim: 640
      dim: 640
      stride: 1
      stride: 1
      offset: 0
      offset: 0
    }
    padding: REFLECT
    #Min and Max sizes. This will be applied before the rest of the processing.
    #Setting both of these may cause distortion of the image. Recommed to set only one of these or a large range.
    #size_min: 512    
    #size_max: 2048   
    
    #Random scaling before crop    
    scale_prob: 0.5
    scale_min: 0.75
    scale_max: 1.25    
    
    #Set to 1 to disable multireaded read      
    #threads: 1           
  }
}

layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  include {  phase: TEST  }
  transform_param {
    crop_size: 640
    mirror: false       
    #mean_value: 128
    #mean_value: 128
    #mean_value: 128
  }
  image_label_data_param {    
    image_list_path: "data/val-image-list.txt"
    label_list_path: "data/val-label-list.txt"
    
    batch_size: 4
    shuffle: true
    label_slice {
      dim: 640
      dim: 640
      stride: 1
      stride: 1
      offset: 0
      offset: 0
    }
    padding: REFLECT
  }
}

layer {
  name: "data_bias"
  bottom: "data"
  top: "data_bias"
  type: "Bias"
  param { #scale
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}

layer {
  name: "conv1a"
  bottom: "data_bias"
  top: "conv1a"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 32
    kernel_size: 5
    pad: 2
    stride: 2
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 1
  }
}
layer {
  name: "bn_conv1a"
  bottom: "conv1a"
  top: "conv1a/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "conv1a_relu"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
  type: "ReLU"
}
layer {
  name: "conv1b"
  bottom: "conv1a/bn"
  top: "conv1b"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 32
    kernel_size: 3
    pad: 1
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 4 #1
  }
}
layer {
  name: "bn_conv1b"
  bottom: "conv1b"
  top: "conv1b/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "conv1b_relu"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
  type: "ReLU"
}
layer {
  name: "res2a_branch2a"
  bottom: "conv1b/bn"
  top: "res2a_branch2a"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 1
    stride: 2
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 1 #4
  }
}
layer {
  name: "bn2a_branch2a"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res2a_branch2a_relu"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
  type: "ReLU"
}
layer {
  name: "res2a_branch2b"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 1
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 4
  }
}
layer {
  name: "bn2a_branch2b"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res2a_relu"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
  type: "ReLU"
}
layer {
  name: "res3a_branch2a"
  bottom: "res2a_branch2b/bn"
  top: "res3a_branch2a"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 128
    kernel_size: 3
    pad: 1
    stride: 2
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 1 #4
  }
}
layer {
  name: "bn3a_branch2a"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res3a_branch2a_relu"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
  type: "ReLU"
}
layer {
  name: "res3a_branch2b"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 128
    kernel_size: 3
    pad: 1
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 4
  }
}
layer {
  name: "bn3a_branch2b"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res3a_relu"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
  type: "ReLU"
}
layer {
  name: "res4a_branch2a"
  bottom: "res3a_branch2b/bn"
  top: "res4a_branch2a"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 256
    kernel_size: 3
    pad: 1
    stride: 2
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 1 #4
  }
}
layer {
  name: "bn4a_branch2a"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res4a_branch2a_relu"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
  type: "ReLU"
}
layer {
  name: "res4a_branch2b"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 256
    kernel_size: 3
    pad: 1
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 4
  }
}
layer {
  name: "bn4a_branch2b"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res4a_relu"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
  type: "ReLU"
}
layer {
  name: "res5a_branch2a"
  bottom: "res4a_branch2b/bn"
  top: "res5a_branch2a"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 1 #4
  }
}
layer {
  name: "bn5a_branch2a"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res5a_branch2a_relu"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
  type: "ReLU"
}
layer {
  name: "res5a_branch2b"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 2
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 2    
    group: 4
  }
}
layer {
  name: "bn5a_branch2b"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res5a_relu"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
  type: "ReLU"
}

#------------------------------------------------------
#Output layers
#------------------------------------------------------
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 4
    bias_term: true    
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 } 
    dilation: 4
    group: 2 
  }  
}
layer {
	bottom: "out5a"
	top: "out5a/bn"
	name: "bn_out5a"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0
      }
      moving_average_fraction: 0.99
      eps: 0.0001        
    }
}
layer {
  name: "out5a_relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"  
}
layer {
  name: "out5_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }    
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 1
    bias_term: true    
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 } 
    dilation: 1
    group: 2
  }  
}
layer {
	bottom: "out3a"
	top: "out3a/bn"
	name: "bn_out3a"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0
      }
      moving_average_fraction: 0.99
      eps: 0.0001        
    }
}
layer {
  name: "out3a_relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
	name: "out3_out5_combined"
	bottom: "out5_up2"
	bottom: "out3a/bn"
	top: "out3_out5_combined"
	type: "Eltwise"
}

#----------------------------------------------------------------------------
#Additional ctx layers
#----------------------------------------------------------------------------
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param { lr_mult: 1 decay_mult: 1.0 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    bias_term: true       
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }  
    dilation: 1
    group: 1     
  }  
}

layer {
	bottom: "ctx_conv1"
	top: "ctx_conv1/bn"
	name: "bn_ctx_conv1"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0
      }
      moving_average_fraction: 0.99
      eps: 0.0001       
    }
}

layer {
  name: "ctx_relu1"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}

layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param { lr_mult: 1 decay_mult: 1.0 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    pad: 4
    kernel_size: 3
    bias_term: true       
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 } 
    dilation: 4
    group: 1     
  } 
}

layer {
	bottom: "ctx_conv2"
	top: "ctx_conv2/bn"
	name: "bn_ctx_conv2"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      moving_average_fraction: 0.99
      eps: 0.0001       
    }
}

layer {
  name: "ctx_relu2"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn" 
}

layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param { lr_mult: 1 decay_mult: 1.0 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    pad: 4
    kernel_size: 3
    bias_term: true       
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 } 
    dilation: 4
    group: 1      
  }
}

layer {
	bottom: "ctx_conv3"
	top: "ctx_conv3/bn"
	name: "bn_ctx_conv3"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0
      }
      moving_average_fraction: 0.99
      eps: 0.0001       
    }
}


layer {
  name: "ctx_relu3"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}

layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param { lr_mult: 1 decay_mult: 1.0 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    pad: 4
    kernel_size: 3
    bias_term: true       
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 } 
    dilation: 4
    group: 1        
  }  
}

layer {
	bottom: "ctx_conv4"
	top: "ctx_conv4/bn"
	name: "bn_ctx_conv4"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0
      }
      moving_average_fraction: 0.99
      eps: 0.0001       
    }
}

layer {
  name: "ctx_conv4_relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"  
}

layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param { lr_mult: 1 decay_mult: 1.0 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 8
    kernel_size: 3
    pad: 1
    bias_term: true       
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }   
    dilation: 1   
    group: 1      
  }   
}

layer {
  name: "ctx_final_relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"     
}

#--------------------------------------------
#Final deconvolution layers
#--------------------------------------------
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }    
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }    
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }    
  }
}
#----
layer {
  name: "ctx_final_score"
  type: "Crop"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "ctx_final_score"
  crop_param {
    axis: 2
    offset: 0
  }
}

#--------------------------------------------
#Score/Loss/Accuracy of final layer 
#--------------------------------------------
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ctx_final_score"
  bottom: "label"
  top: "loss"
  #loss_weight: 1.0
  loss_param {
    ignore_label: 255
    normalization: VALID
    #assign_label_weights: true
    #num_label_weights: 5
    #bootstrap_prob_threshold: 0.9
    #bootstrap_samples_fraction: 0.01
  }
  accuracy_param {
    ignore_label: 255
  }    
}

layer {
  name: "iou"
  type: "IOUAccuracy"
  bottom: "ctx_final_score"
  bottom: "label"
  top: "iou/mean"
  top: "iou/class"  
  accuracy_param {
    ignore_label: 255
    history_size: 1000
  }  
  include {
    phase: TEST
  }
}

layer {
  name: "pixel_accuracy/top-1"
  type: "Accuracy"
  bottom: "ctx_final_score"
  bottom: "label"
  top: "pixel_accuracy/top-1"
  accuracy_param {
    ignore_label: 255
  }  
  include {
    phase: TEST
  }
}

}


